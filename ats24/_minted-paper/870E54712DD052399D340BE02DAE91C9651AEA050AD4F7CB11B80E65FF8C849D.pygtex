\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{Function}\PYG{p}{(}\PYG{n}{\PYGZus{}C}\PYG{o}{.}\PYG{n}{\PYGZus{}FunctionBase}\PYG{p}{):}
  \PYG{n+nd}{@staticmethod}
  \PYG{k}{def} \PYG{n+nf}{backward}\PYG{p}{(}\PYG{n}{ctx}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{,} \PYG{o}{*}\PYG{n}{grad\PYGZus{}outputs}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{)} \PYG{o}{\PYGZhy{}\PYGZgt{}} \PYG{n}{Any}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{NotImplementedError}\PYG{p}{(}
      \PYG{l+s+s2}{\PYGZdq{}You must implement your custom autograd.Function\PYGZdq{}}
    \PYG{p}{)}

\PYG{k}{class} \PYG{n+nc}{fake\PYGZus{}quantize\PYGZus{}function}\PYG{p}{(}\PYG{n}{Function}\PYG{p}{):}
  \PYG{n+nd}{@staticmethod}
  \PYG{k}{def} \PYG{n+nf}{backward}\PYG{p}{(}\PYG{n}{ctx}\PYG{p}{,} \PYG{n}{grad\PYGZus{}output}\PYG{p}{:} \PYG{n}{Tensor}\PYG{p}{)} \PYG{o}{\PYGZhy{}\PYGZgt{}} \PYG{n}{Tensor}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{grad\PYGZus{}output} \PYG{o}{*} \PYG{n}{ctx}\PYG{o}{.}\PYG{n}{saved\PYGZus{}tensors}
\end{Verbatim}
